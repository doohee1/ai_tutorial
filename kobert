!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart
!pip install transformers

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import drive

drive.mount("/content/gdrive")
%cd /content/gdrive/MyDrive/WAW

df = pd.read_excel('/content/gdrive/MyDrive/뉴스 통합.xlsx')
content = df['Content']

# load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")
model = AutoModelForSeq2SeqLM.from_pretrained("digit82/kobart-summarization")

model.config.num_labels = 2

# summaries function
def generate_summary(text):
    raw_input_ids = tokenizer.encode(text,max_length=512, truncation=True)
    input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]
    summary_ids = model.generate(torch.tensor([input_ids]), num_beams=2, max_length=512, eos_token_id=1)
    summary = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)
    return summary

summaries = [generate_summary(text) for text in content]

df['Summary'] = summaries
#Summary 열 추가하여 새로운 파일 저장
df.to_excel('/content/gdrive/MyDrive/summarized_data.xlsx', index=False)
